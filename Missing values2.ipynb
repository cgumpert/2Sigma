{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first: I have some data analysis background from my research career in particle physics, but I am far from being an machine learning expert. So please bear with me and I am happy to receive any kind of feedback.\n",
    "\n",
    "Since the training data set (and possibly the test data as well) contain missing data, I wanted to have a closer look at this issue. I have seen that other participants propose to fill those NaNs with the mean or median for the respective column. Here I am not (yet) that much interested in filling the blanks but I rather want to know whether we can learn something more about the data when looking missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with pd.HDFStore('train.h5') as train:\n",
    "    df = train.get('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how much data we've got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very interesting. This means that rows with missing data for a given ID and feature are always continuous in time. It is **not** the case that data for an asset is unavailable, becomes available and then becomes unavailable again. I bet that if data is unavailable, it is at the beginning of the time interval in which the corresponding asset is traded. Let's try to visualize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# iterate over IDs\n",
    "grouped = df.groupby('id');\n",
    "for i,(n,g) in enumerate(grouped):\n",
    "    # get missing data flag for feature columns\n",
    "    d = g.isnull().drop(['timestamp','id','y'],axis=1)\n",
    "    # normalise time stamp to start with 0 when ID appears for the first time in portfolio\n",
    "    d.index = g.timestamp - g.timestamp.min()\n",
    "    d.index.name = 'relative timestamp'\n",
    "    plt.figure(figsize=(16,12))\n",
    "    plt.title(\"ID = %d\" % n)\n",
    "    sns.heatmap(d.T,xticklabels=100,cbar=False)\n",
    "    # only plot first 10 IDs\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Bingo! Black bars denote columns and time stamps with missing data. All bars start at the very left which is the time an ID appears for the first time in the portfolio. It also looks like there are different *kinds* of these _barcode plots_. Maybe one can try to categorise IDs based on their missing value patterns... I will post an update on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ids = sorted(df.id.unique())\n",
    "columns = df.columns.drop(['id','timestamp','y']).insert(0,'size')\n",
    "nan_df = pd.DataFrame(data=None,index=ids,columns=columns,dtype=float)\n",
    "# iterate over all asset ID\n",
    "for name,group in df.groupby('id'):\n",
    "    # for every feature column\n",
    "    for c in columns:\n",
    "        if c == 'size':\n",
    "            nan_df.loc[name,c] = int(len(group))\n",
    "        else:\n",
    "            # total number of rows with missing data\n",
    "            nan_df.loc[name,c] = float(group[c].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nan_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binary = nan_df.drop(['size'],axis=1)\n",
    "binary[binary > 0] = 1\n",
    "binary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import dbscan\n",
    "_,labels = dbscan(binary.values,eps=3,min_samples=10)\n",
    "plt.hist(labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fractional = nan_df.div(nan_df['size'],axis='index').drop(['size'],axis=1)\n",
    "fractional.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_,labels = dbscan(fractional.values,min_samples=10)\n",
    "plt.hist(labels,range=(-1.5,labels.max()+0.5),bins=labels.max()+2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fractional2 = nan_df.copy()\n",
    "fractional2[fractional2 > 100] = 100\n",
    "fractional2['size'] = nan_df['size']\n",
    "fractional2 = fractional2.div(fractional2['size'],axis='index').drop(['size'],axis=1)\n",
    "fractional2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_,labels = dbscan(fractional2.values,eps=2,min_samples=10)\n",
    "plt.hist(labels,range=(-1.5,labels.max()+0.5),bins=labels.max()+2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fractional2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "#Range for k\n",
    "kmin = 2\n",
    "kmax = 25\n",
    "sil_scores = []\n",
    "\n",
    "#Compute silouhette scoeres\n",
    "for k in range(kmin,kmax):\n",
    "    km = KMeans(n_clusters=k).fit(fractional.values)\n",
    "    sil_scores.append(silhouette_score(fractional.values, km.labels_))\n",
    "\n",
    "#Plot\n",
    "plt.plot(range(kmin,kmax), sil_scores)\n",
    "plt.title('KMeans Results')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()\n",
    "#_,labels,_ = k_means(fractional.values,n_clusters=10)\n",
    "#plt.hist(labels,range=(-1.5,labels.max()+0.5),bins=labels.max()+2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = df.groupby('timestamp').apply(lambda x: x.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = (t - t.shift()).drop(['id','timestamp','y'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = df.groupby('timestamp').apply(len)\n",
    "b = (n - n.shift())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a.plot(figsize=(24,12))\n",
    "b.plot(style=['.b'],ax=plt.gca())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a[:350].plot(figsize=(16,8))\n",
    "b[:350].plot(style=['.b'],ax=plt.gca())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_lag = 250\n",
    "corrs = np.zeros((max_lag,a.shape[1]))\n",
    "for l in range(1,max_lag):\n",
    "    c = a.shift(-l)\n",
    "    corrs[l] = c.corrwith(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,18))\n",
    "sns.heatmap(corrs,xticklabels=a.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.shift(6).corrwith(b).filter(regex=\"technical_*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
